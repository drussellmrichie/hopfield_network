{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an efficient algorithm for the Storkey Learning Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1, -1,  1, -1],\n",
       "        [-1, -1, -1,  1],\n",
       "        [-1, -1,  1,  1],\n",
       "        [-1,  1, -1,  1]],\n",
       "\n",
       "       [[ 1, -1,  1, -1],\n",
       "        [ 1, -1, -1, -1],\n",
       "        [-1,  1, -1, -1],\n",
       "        [-1, -1, -1, -1]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nunits = 4\n",
    "npats = 2\n",
    "pats = 2 * np.random.binomial(1, 0.5, [npats, nunits, nunits]) - 1\n",
    "pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1083efb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pat = pats[1]\n",
    "\n",
    "plt.imshow(test_pat, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the weights. **Whereas before we used Hebb's rule, now let's use the Storkey Learning Rule**. This rule has a few nice advantages over Hebb's rule: it allows the network to learn more patterns, its basins of attraction (to the stored patterns) are larger, the distribution of basin sizes is more even, and the shapes of the basins are more round. The weights at time `v` are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e0b194a405a470e54aacef9e75ced89d02f60844)\n",
    "\n",
    "where\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0e9c85f5bbf569acdfc8dd7ab8cccb742a0f856)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `n` is the number of neurons and $\\epsilon$ is a bit (+1 or -1) of pattern `v`.\n",
    "\n",
    "The second term of the rule is basically the Hebbian rule. The third and fourth terms basically account for the net input to neurons j and i using the current weights.\n",
    "\n",
    "Here is the algorithm I settled on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storkey_rule(pattern, old_weights=None):\n",
    "    \"\"\"\n",
    "    pattern: 2-dimensional array\n",
    "    old_weights: square array of length pattern.shape[0]*pattern.shape[1]\n",
    "    \"\"\"\n",
    "    \n",
    "    mem = pattern.flatten()    \n",
    "    n = len(mem)\n",
    "    \n",
    "    if old_weights is None:\n",
    "        old_weights = np.zeros(shape=(n,n))\n",
    "\n",
    "    hebbian_term  = np.outer(mem,mem)\n",
    "    \n",
    "    net_inputs = old_weights.dot(mem)\n",
    "    net_inputs = np.tile(net_inputs, (n, 1)) # repeat the net_input vector n times along the rows \n",
    "                                             # so we now have a matrix\n",
    "    \n",
    "    # h_i and h_j should exclude input from i and j from h_ij\n",
    "    h_i = np.diagonal(old_weights) * mem # this obtains the input each neuron receives from itself\n",
    "    h_i = h_i[:, np.newaxis]             # turn h_i into a column vector so we can subtract from hij appropriately\n",
    "    \n",
    "    h_j = old_weights * mem              # element-wise multiply each row of old-weights by mem    \n",
    "    np.fill_diagonal(h_j,0)              # now replace the diagonal of h_j with 0's; the diagonal of h_j is the \n",
    "                                         # self-inputs, which are redundant with h_i\n",
    "    \n",
    "    hij = net_inputs - h_i - h_j\n",
    "    \n",
    "    post_synaptic  = hij * mem\n",
    "    #pre_synaptic = post_synaptic.T\n",
    "        \n",
    "    #new_weights = old_weights + (1./n)*(hebbian_term - pre_synaptic - post_synaptic)\n",
    "    new_weights = old_weights + (1./n)*(hebbian_term - 2*post_synaptic)\n",
    "    \n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That algorithm is probably fairly opaque, owing to the matrix operations in it. Let's unpack it step-by-step. It's most illustrative to look at what happens when you are training a second pattern, because the weights of the first pattern are simply due to the hebbian term -- the other terms are just 0's. So, we'll start with `old_weights` from training the first test pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "old_weights = storkey_rule(pats[0])\n",
    "\n",
    "pattern = pats[1]\n",
    "\n",
    "mem = pattern.flatten()\n",
    "print(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "n = len(mem)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1  1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [ 1 -1  1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [ 1 -1  1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [ 1 -1  1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1  1  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "if old_weights is None:\n",
    "    old_weights = np.zeros(shape=(n,n))\n",
    "\n",
    "hebbian_term  = np.outer(mem,mem)\n",
    "print(hebbian_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "net_inputs = old_weights.dot(mem)\n",
    "print(net_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "net_inputs = np.tile(net_inputs, (n, 1)) # repeat the net_input vector n times along the rows \n",
    "                                         # so we now have a matrix\n",
    "print(net_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0625]\n",
      " [-0.0625]\n",
      " [ 0.0625]\n",
      " [-0.0625]\n",
      " [ 0.0625]\n",
      " [-0.0625]\n",
      " [-0.0625]\n",
      " [-0.0625]\n",
      " [-0.0625]\n",
      " [ 0.0625]\n",
      " [-0.0625]\n",
      " [-0.0625]\n",
      " [-0.0625]\n",
      " [-0.0625]\n",
      " [-0.0625]\n",
      " [-0.0625]]\n"
     ]
    }
   ],
   "source": [
    "# h_i and h_j should exclude input from i and j from h_ij\n",
    "h_i = np.diagonal(old_weights) * mem # this obtains the input each neuron receives from itself\n",
    "h_i = h_i[:, np.newaxis]             # turn h_i into a column vector so we can subtract from hij appropriately\n",
    "print(h_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     -0.0625 -0.0625 -0.0625  0.0625 -0.0625 -0.0625  0.0625 -0.0625\n",
      "   0.0625  0.0625  0.0625 -0.0625  0.0625 -0.0625  0.0625]\n",
      " [ 0.0625  0.     -0.0625 -0.0625  0.0625 -0.0625 -0.0625  0.0625 -0.0625\n",
      "   0.0625  0.0625  0.0625 -0.0625  0.0625 -0.0625  0.0625]\n",
      " [-0.0625  0.0625  0.      0.0625 -0.0625  0.0625  0.0625 -0.0625  0.0625\n",
      "  -0.0625 -0.0625 -0.0625  0.0625 -0.0625  0.0625 -0.0625]\n",
      " [ 0.0625 -0.0625 -0.0625  0.      0.0625 -0.0625 -0.0625  0.0625 -0.0625\n",
      "   0.0625  0.0625  0.0625 -0.0625  0.0625 -0.0625  0.0625]\n",
      " [ 0.0625 -0.0625 -0.0625 -0.0625  0.     -0.0625 -0.0625  0.0625 -0.0625\n",
      "   0.0625  0.0625  0.0625 -0.0625  0.0625 -0.0625  0.0625]\n",
      " [ 0.0625 -0.0625 -0.0625 -0.0625  0.0625  0.     -0.0625  0.0625 -0.0625\n",
      "   0.0625  0.0625  0.0625 -0.0625  0.0625 -0.0625  0.0625]\n",
      " [ 0.0625 -0.0625 -0.0625 -0.0625  0.0625 -0.0625  0.      0.0625 -0.0625\n",
      "   0.0625  0.0625  0.0625 -0.0625  0.0625 -0.0625  0.0625]\n",
      " [-0.0625  0.0625  0.0625  0.0625 -0.0625  0.0625  0.0625  0.      0.0625\n",
      "  -0.0625 -0.0625 -0.0625  0.0625 -0.0625  0.0625 -0.0625]\n",
      " [ 0.0625 -0.0625 -0.0625 -0.0625  0.0625 -0.0625 -0.0625  0.0625  0.\n",
      "   0.0625  0.0625  0.0625 -0.0625  0.0625 -0.0625  0.0625]\n",
      " [ 0.0625 -0.0625 -0.0625 -0.0625  0.0625 -0.0625 -0.0625  0.0625 -0.0625\n",
      "   0.      0.0625  0.0625 -0.0625  0.0625 -0.0625  0.0625]\n",
      " [-0.0625  0.0625  0.0625  0.0625 -0.0625  0.0625  0.0625 -0.0625  0.0625\n",
      "  -0.0625  0.     -0.0625  0.0625 -0.0625  0.0625 -0.0625]\n",
      " [-0.0625  0.0625  0.0625  0.0625 -0.0625  0.0625  0.0625 -0.0625  0.0625\n",
      "  -0.0625 -0.0625  0.      0.0625 -0.0625  0.0625 -0.0625]\n",
      " [ 0.0625 -0.0625 -0.0625 -0.0625  0.0625 -0.0625 -0.0625  0.0625 -0.0625\n",
      "   0.0625  0.0625  0.0625  0.      0.0625 -0.0625  0.0625]\n",
      " [-0.0625  0.0625  0.0625  0.0625 -0.0625  0.0625  0.0625 -0.0625  0.0625\n",
      "  -0.0625 -0.0625 -0.0625  0.0625  0.      0.0625 -0.0625]\n",
      " [ 0.0625 -0.0625 -0.0625 -0.0625  0.0625 -0.0625 -0.0625  0.0625 -0.0625\n",
      "   0.0625  0.0625  0.0625 -0.0625  0.0625  0.      0.0625]\n",
      " [-0.0625  0.0625  0.0625  0.0625 -0.0625  0.0625  0.0625 -0.0625  0.0625\n",
      "  -0.0625 -0.0625 -0.0625  0.0625 -0.0625  0.0625  0.    ]]\n"
     ]
    }
   ],
   "source": [
    "h_j = old_weights * mem              # element-wise multiply each row of old-weights by mem\n",
    "np.fill_diagonal(h_j,0)\n",
    "print(h_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0625  0.      0.      0.     -0.125   0.      0.     -0.125   0.\n",
      "  -0.125  -0.125  -0.125   0.     -0.125   0.     -0.125 ]\n",
      " [ 0.      0.0625  0.125   0.125   0.      0.125   0.125   0.      0.125\n",
      "   0.      0.      0.      0.125   0.      0.125   0.    ]\n",
      " [ 0.     -0.125  -0.0625 -0.125   0.     -0.125  -0.125   0.     -0.125\n",
      "   0.      0.      0.     -0.125   0.     -0.125   0.    ]\n",
      " [ 0.      0.125   0.125   0.0625  0.      0.125   0.125   0.      0.125\n",
      "   0.      0.      0.      0.125   0.      0.125   0.    ]\n",
      " [-0.125   0.      0.      0.     -0.0625  0.      0.     -0.125   0.\n",
      "  -0.125  -0.125  -0.125   0.     -0.125   0.     -0.125 ]\n",
      " [ 0.      0.125   0.125   0.125   0.      0.0625  0.125   0.      0.125\n",
      "   0.      0.      0.      0.125   0.      0.125   0.    ]\n",
      " [ 0.      0.125   0.125   0.125   0.      0.125   0.0625  0.      0.125\n",
      "   0.      0.      0.      0.125   0.      0.125   0.    ]\n",
      " [ 0.125   0.      0.      0.      0.125   0.      0.      0.0625  0.\n",
      "   0.125   0.125   0.125   0.      0.125   0.      0.125 ]\n",
      " [ 0.      0.125   0.125   0.125   0.      0.125   0.125   0.      0.0625\n",
      "   0.      0.      0.      0.125   0.      0.125   0.    ]\n",
      " [-0.125   0.      0.      0.     -0.125   0.      0.     -0.125   0.\n",
      "  -0.0625 -0.125  -0.125   0.     -0.125   0.     -0.125 ]\n",
      " [ 0.125   0.      0.      0.      0.125   0.      0.      0.125   0.\n",
      "   0.125   0.0625  0.125   0.      0.125   0.      0.125 ]\n",
      " [ 0.125   0.      0.      0.      0.125   0.      0.      0.125   0.\n",
      "   0.125   0.125   0.0625  0.      0.125   0.      0.125 ]\n",
      " [ 0.      0.125   0.125   0.125   0.      0.125   0.125   0.      0.125\n",
      "   0.      0.      0.      0.0625  0.      0.125   0.    ]\n",
      " [ 0.125   0.      0.      0.      0.125   0.      0.      0.125   0.\n",
      "   0.125   0.125   0.125   0.      0.0625  0.      0.125 ]\n",
      " [ 0.      0.125   0.125   0.125   0.      0.125   0.125   0.      0.125\n",
      "   0.      0.      0.      0.125   0.      0.0625  0.    ]\n",
      " [ 0.125   0.      0.      0.      0.125   0.      0.      0.125   0.\n",
      "   0.125   0.125   0.125   0.      0.125   0.      0.0625]]\n"
     ]
    }
   ],
   "source": [
    "hij = net_inputs - h_i - h_j\n",
    "print(hij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0625 -0.      0.     -0.     -0.125  -0.     -0.      0.125  -0.\n",
      "  -0.125   0.125   0.125  -0.      0.125  -0.      0.125 ]\n",
      " [ 0.     -0.0625  0.125  -0.125   0.     -0.125  -0.125  -0.     -0.125\n",
      "   0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [ 0.      0.125  -0.0625  0.125   0.      0.125   0.125  -0.      0.125\n",
      "   0.     -0.     -0.      0.125  -0.      0.125  -0.    ]\n",
      " [ 0.     -0.125   0.125  -0.0625  0.     -0.125  -0.125  -0.     -0.125\n",
      "   0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [-0.125  -0.      0.     -0.     -0.0625 -0.     -0.      0.125  -0.\n",
      "  -0.125   0.125   0.125  -0.      0.125  -0.      0.125 ]\n",
      " [ 0.     -0.125   0.125  -0.125   0.     -0.0625 -0.125  -0.     -0.125\n",
      "   0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [ 0.     -0.125   0.125  -0.125   0.     -0.125  -0.0625 -0.     -0.125\n",
      "   0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [ 0.125  -0.      0.     -0.      0.125  -0.     -0.     -0.0625 -0.\n",
      "   0.125  -0.125  -0.125  -0.     -0.125  -0.     -0.125 ]\n",
      " [ 0.     -0.125   0.125  -0.125   0.     -0.125  -0.125  -0.     -0.0625\n",
      "   0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [-0.125  -0.      0.     -0.     -0.125  -0.     -0.      0.125  -0.\n",
      "  -0.0625  0.125   0.125  -0.      0.125  -0.      0.125 ]\n",
      " [ 0.125  -0.      0.     -0.      0.125  -0.     -0.     -0.125  -0.\n",
      "   0.125  -0.0625 -0.125  -0.     -0.125  -0.     -0.125 ]\n",
      " [ 0.125  -0.      0.     -0.      0.125  -0.     -0.     -0.125  -0.\n",
      "   0.125  -0.125  -0.0625 -0.     -0.125  -0.     -0.125 ]\n",
      " [ 0.     -0.125   0.125  -0.125   0.     -0.125  -0.125  -0.     -0.125\n",
      "   0.     -0.     -0.     -0.0625 -0.     -0.125  -0.    ]\n",
      " [ 0.125  -0.      0.     -0.      0.125  -0.     -0.     -0.125  -0.\n",
      "   0.125  -0.125  -0.125  -0.     -0.0625 -0.     -0.125 ]\n",
      " [ 0.     -0.125   0.125  -0.125   0.     -0.125  -0.125  -0.     -0.125\n",
      "   0.     -0.     -0.     -0.125  -0.     -0.0625 -0.    ]\n",
      " [ 0.125  -0.      0.     -0.      0.125  -0.     -0.     -0.125  -0.\n",
      "   0.125  -0.125  -0.125  -0.     -0.125  -0.     -0.0625]]\n"
     ]
    }
   ],
   "source": [
    "post_synaptic  = hij * mem\n",
    "print(post_synaptic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0625  0.      0.      0.     -0.125   0.      0.      0.125   0.\n",
      "  -0.125   0.125   0.125   0.      0.125   0.      0.125 ]\n",
      " [-0.     -0.0625  0.125  -0.125  -0.     -0.125  -0.125  -0.     -0.125\n",
      "  -0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [ 0.      0.125  -0.0625  0.125   0.      0.125   0.125   0.      0.125\n",
      "   0.      0.      0.      0.125   0.      0.125   0.    ]\n",
      " [-0.     -0.125   0.125  -0.0625 -0.     -0.125  -0.125  -0.     -0.125\n",
      "  -0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [-0.125   0.      0.      0.     -0.0625  0.      0.      0.125   0.\n",
      "  -0.125   0.125   0.125   0.      0.125   0.      0.125 ]\n",
      " [-0.     -0.125   0.125  -0.125  -0.     -0.0625 -0.125  -0.     -0.125\n",
      "  -0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [-0.     -0.125   0.125  -0.125  -0.     -0.125  -0.0625 -0.     -0.125\n",
      "  -0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [ 0.125  -0.     -0.     -0.      0.125  -0.     -0.     -0.0625 -0.\n",
      "   0.125  -0.125  -0.125  -0.     -0.125  -0.     -0.125 ]\n",
      " [-0.     -0.125   0.125  -0.125  -0.     -0.125  -0.125  -0.     -0.0625\n",
      "  -0.     -0.     -0.     -0.125  -0.     -0.125  -0.    ]\n",
      " [-0.125   0.      0.      0.     -0.125   0.      0.      0.125   0.\n",
      "  -0.0625  0.125   0.125   0.      0.125   0.      0.125 ]\n",
      " [ 0.125  -0.     -0.     -0.      0.125  -0.     -0.     -0.125  -0.\n",
      "   0.125  -0.0625 -0.125  -0.     -0.125  -0.     -0.125 ]\n",
      " [ 0.125  -0.     -0.     -0.      0.125  -0.     -0.     -0.125  -0.\n",
      "   0.125  -0.125  -0.0625 -0.     -0.125  -0.     -0.125 ]\n",
      " [-0.     -0.125   0.125  -0.125  -0.     -0.125  -0.125  -0.     -0.125\n",
      "  -0.     -0.     -0.     -0.0625 -0.     -0.125  -0.    ]\n",
      " [ 0.125  -0.     -0.     -0.      0.125  -0.     -0.     -0.125  -0.\n",
      "   0.125  -0.125  -0.125  -0.     -0.0625 -0.     -0.125 ]\n",
      " [-0.     -0.125   0.125  -0.125  -0.     -0.125  -0.125  -0.     -0.125\n",
      "  -0.     -0.     -0.     -0.125  -0.     -0.0625 -0.    ]\n",
      " [ 0.125  -0.     -0.     -0.      0.125  -0.     -0.     -0.125  -0.\n",
      "   0.125  -0.125  -0.125  -0.     -0.125  -0.     -0.0625]]\n"
     ]
    }
   ],
   "source": [
    "pre_synaptic = post_synaptic.T #equivalent to np.outer(hij,mem)\n",
    "print(pre_synaptic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1328125  0.         0.         0.         0.140625   0.         0.\n",
      "  -0.140625   0.         0.140625  -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [ 0.         0.1328125 -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.        -0.140625   0.1328125 -0.140625   0.        -0.140625\n",
      "  -0.140625   0.        -0.140625   0.         0.         0.        -0.140625\n",
      "   0.        -0.140625   0.       ]\n",
      " [ 0.         0.140625  -0.140625   0.1328125  0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.140625   0.         0.         0.         0.1328125  0.         0.\n",
      "  -0.140625   0.         0.140625  -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.1328125\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.1328125  0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.1328125  0.        -0.140625   0.140625   0.140625   0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.1328125  0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.140625   0.         0.         0.         0.140625   0.         0.\n",
      "  -0.140625   0.         0.1328125 -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.1328125  0.140625   0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.1328125  0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.1328125\n",
      "   0.         0.140625   0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.140625   0.         0.1328125\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.1328125  0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.140625   0.         0.140625\n",
      "   0.         0.1328125]]\n"
     ]
    }
   ],
   "source": [
    "new_weights = old_weights + (1./n)*(hebbian_term - pre_synaptic - post_synaptic)\n",
    "print(new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's verify that that algorithms works (I'm kinda shaky on my linear algebra). Let's verify it against an inefficient, but more transparent, for-loops implementation...\n",
    "\n",
    "Again, the formula for the Storkey rule:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e0b194a405a470e54aacef9e75ced89d02f60844)\n",
    "\n",
    "where\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0e9c85f5bbf569acdfc8dd7ab8cccb742a0f856)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storkey_rule_loops(patterns):\n",
    "    memories = [x.flatten() for x in patterns]\n",
    "    neurons  = len(memories[0])\n",
    "    memMat = np.zeros((neurons,neurons))\n",
    "    for mem in memories:\n",
    "        oldMemMat = memMat.copy()\n",
    "        for i in range(neurons):\n",
    "            for j in range(neurons):\n",
    "                hij = np.sum([ oldMemMat[i,k]*mem[k] for k in range(neurons) if k not in [i,j] ])\n",
    "                hji = np.sum([ oldMemMat[j,k]*mem[k] for k in range(neurons) if k not in [i,j] ])\n",
    "                memMat[i,j] += (1./neurons)*(mem[i]*mem[j] - mem[i]*hji - mem[j]*hij)\n",
    "    return memMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 15.24 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 134 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit storkey_rule(pattern=pats[1], old_weights=storkey_rule(pats[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 32 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit storkey_rule_loops(pats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the matrix math version -- which uses the optimized numpy data structures and operations -- is much, much faster than the for-loops verison. Now compare the weights of the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1328125  0.         0.         0.         0.140625   0.         0.\n",
      "  -0.140625   0.         0.140625  -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [ 0.         0.1328125 -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.        -0.140625   0.1328125 -0.140625   0.        -0.140625\n",
      "  -0.140625   0.        -0.140625   0.         0.         0.        -0.140625\n",
      "   0.        -0.140625   0.       ]\n",
      " [ 0.         0.140625  -0.140625   0.1328125  0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.140625   0.         0.         0.         0.1328125  0.         0.\n",
      "  -0.140625   0.         0.140625  -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.1328125\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.1328125  0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.1328125  0.        -0.140625   0.140625   0.140625   0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.1328125  0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.140625   0.         0.         0.         0.140625   0.         0.\n",
      "  -0.140625   0.         0.1328125 -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.1328125  0.140625   0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.1328125  0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.1328125\n",
      "   0.         0.140625   0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.140625   0.         0.1328125\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.1328125  0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.140625   0.         0.140625\n",
      "   0.         0.1328125]]\n"
     ]
    }
   ],
   "source": [
    "loop_weights = storkey_rule_loops(pats)\n",
    "print(loop_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1328125  0.         0.         0.         0.140625   0.         0.\n",
      "  -0.140625   0.         0.140625  -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [ 0.         0.1328125 -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.        -0.140625   0.1328125 -0.140625   0.        -0.140625\n",
      "  -0.140625   0.        -0.140625   0.         0.         0.        -0.140625\n",
      "   0.        -0.140625   0.       ]\n",
      " [ 0.         0.140625  -0.140625   0.1328125  0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.140625   0.         0.         0.         0.1328125  0.         0.\n",
      "  -0.140625   0.         0.140625  -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.1328125\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.1328125  0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.1328125  0.        -0.140625   0.140625   0.140625   0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.1328125  0.         0.         0.         0.140625\n",
      "   0.         0.140625   0.       ]\n",
      " [ 0.140625   0.         0.         0.         0.140625   0.         0.\n",
      "  -0.140625   0.         0.1328125 -0.140625  -0.140625   0.        -0.140625\n",
      "   0.        -0.140625 ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.1328125  0.140625   0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.1328125  0.         0.140625\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.1328125\n",
      "   0.         0.140625   0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.140625   0.         0.1328125\n",
      "   0.         0.140625 ]\n",
      " [ 0.         0.140625  -0.140625   0.140625   0.         0.140625\n",
      "   0.140625   0.         0.140625   0.         0.         0.         0.140625\n",
      "   0.         0.1328125  0.       ]\n",
      " [-0.140625   0.         0.         0.        -0.140625   0.         0.\n",
      "   0.140625   0.        -0.140625   0.140625   0.140625   0.         0.140625\n",
      "   0.         0.1328125]]\n"
     ]
    }
   ],
   "source": [
    "matrix_weights = storkey_rule(pats[1], old_weights=storkey_rule(pats[0]))\n",
    "print(matrix_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(matrix_weights == loop_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, they are the same!\n",
    "\n",
    "Now let's show that the weights work, i.e., that feeding the network a noisy version of a trained pattern recovers the trained pattern itself. We'll just flip a certain number of random pixels on each row of the trained pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisify(pattern, numb_flipped):\n",
    "\n",
    "    noisy_pattern = pattern.copy()\n",
    "\n",
    "    for idx, row in enumerate(noisy_pattern):\n",
    "        choices = np.random.choice(range(len(row)), numb_flipped)\n",
    "        noisy_pattern[idx,choices] = -noisy_pattern[idx,choices]\n",
    "        \n",
    "    return noisy_pattern\n",
    "\n",
    "noisy_test_pat = noisify(pattern=test_pat, numb_flipped=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1086ee518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(noisy_test_pat, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with that, and use the weights to update it. We'll update the units asynchronously (one at a time), and keep track of the energy of the network every so often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flow(pattern, weights, theta=0, steps = 1000):\n",
    "    \n",
    "    pattern_flat = pattern.flatten()\n",
    "\n",
    "    if isinstance(theta, numbers.Number):\n",
    "        thetas = np.zeros(len(pattern_flat)) + theta\n",
    "    \n",
    "    for step in range(steps):\n",
    "        unit = np.random.randint(low=0, high=(len(pattern_flat)-1))\n",
    "        unit_weights = weights[unit,:]\n",
    "        net_input = np.dot(unit_weights,pattern_flat)\n",
    "        pattern_flat[unit] = 1 if (net_input > thetas[unit]) else -1\n",
    "        \n",
    "        if (step % 200) == 0:\n",
    "            energy = -0.5*np.dot(np.dot(pattern_flat.T,weights),pattern_flat) + np.dot(thetas,pattern_flat)\n",
    "            print(\"Energy at step {:05d} is now {}\".format(step,energy))\n",
    "\n",
    "    evolved_pattern = np.reshape(a=pattern_flat, newshape=(pattern.shape[0],pattern.shape[1]))\n",
    "    \n",
    "    return evolved_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy at step 00000 is now -2.75\n",
      "Energy at step 00200 is now -8.9375\n",
      "Energy at step 00400 is now -8.9375\n",
      "Energy at step 00600 is now -8.9375\n",
      "Energy at step 00800 is now -8.9375\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1089e5748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noisy_test_pat_evolved = flow(noisy_test_pat, matrix_weights)\n",
    "plt.imshow(noisy_test_pat_evolved, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1089b9cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_pat, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lo, the network settles on the trained pattern."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
