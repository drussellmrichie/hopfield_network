{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an efficient algorithm for the Storkey Learning Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunits = 5\n",
    "npats = 2\n",
    "pats = 2 * np.random.binomial(1, 0.5, [npats, nunits, nunits]) - 1\n",
    "pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pat = pats[1]\n",
    "\n",
    "plt.imshow(test_pat, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the weights. **Whereas before we used Hebb's rule, now let's use the Storkey Learning Rule**. This rule has a few nice advantages over Hebb's rule: it allows the network to learn more patterns, its basins of attraction (to the stored patterns) are larger, the distribution of basin sizes is more even, and the shapes of the basins are more round. The weights at time `v` are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e0b194a405a470e54aacef9e75ced89d02f60844)\n",
    "\n",
    "where\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0e9c85f5bbf569acdfc8dd7ab8cccb742a0f856)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `n` is the number of neurons and $\\epsilon$ is a bit (+1 or -1) of pattern `v`.\n",
    "\n",
    "The second term of the rule is basically the Hebbian rule. The third and fourth terms basically account for the net input to neurons j and i using the current weights.\n",
    "\n",
    "Here is the algorithm I settled on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storkey_rule(pattern, old_weights=None):\n",
    "    \"\"\"\n",
    "    pattern: 2-dimensional array\n",
    "    old_weights: square array of length pattern.shape[0]*pattern.shape[1]\n",
    "    \"\"\"\n",
    "    \n",
    "    mem = pattern.flatten()    \n",
    "    n = len(mem)\n",
    "    \n",
    "    if old_weights is None:\n",
    "        old_weights = np.zeros(shape=(n,n))\n",
    "\n",
    "    hebbian_term  = np.outer(mem,mem)\n",
    "    \n",
    "    net_inputs = old_weights.dot(mem)\n",
    "    net_inputs = np.tile(net_inputs, (n, 1)) # repeat the net_input vector n times along the rows \n",
    "                                             # so we now have a matrix\n",
    "    \n",
    "    # h_i and h_j should exclude input from i and j from h_ij\n",
    "    h_i = np.diagonal(old_weights) * mem # this obtains the input each neuron receives from itself\n",
    "    h_i = h_i[:, np.newaxis]             # turn h_i into a column vector so we can subtract from hij appropriately\n",
    "    \n",
    "    h_j = old_weights * mem              # element-wise multiply each row of old-weights by mem    \n",
    "    np.fill_diagonal(h_j,0)              # now replace the diagonal of h_j with 0's; the diagonal of h_j is the \n",
    "                                         # self-inputs, which are redundant with h_i\n",
    "    \n",
    "    hij = net_inputs - h_i - h_j\n",
    "    \n",
    "    post_synaptic  = hij * mem\n",
    "    pre_synaptic   = post_synaptic.T\n",
    "    #pre_synaptic   = hij.T * mem[:, np.newaxis]\n",
    "    \n",
    "    new_weights = old_weights + (1./n)*(hebbian_term - pre_synaptic - post_synaptic)\n",
    "    #new_weights = old_weights + (1./n)*(hebbian_term - 2*post_synaptic)\n",
    "    \n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones(shape=(3,3)) - np.array(range(3))[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That algorithm is probably fairly opaque, owing to the matrix operations in it. Let's unpack it step-by-step. It's most illustrative to look at what happens when you are training a second pattern, because the weights of the first pattern are simply due to the hebbian term -- the other terms are just 0's. So, we'll start with `old_weights` from training the first test pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights = storkey_rule(pats[0])\n",
    "\n",
    "pattern = pats[1]\n",
    "\n",
    "mem = pattern.flatten()\n",
    "print(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(mem)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if old_weights is None:\n",
    "    old_weights = np.zeros(shape=(n,n))\n",
    "\n",
    "hebbian_term  = np.outer(mem,mem)\n",
    "print(hebbian_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_inputs = old_weights.dot(mem)\n",
    "print(net_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_inputs = np.tile(net_inputs, (n, 1)) # repeat the net_input vector n times along the rows \n",
    "                                         # so we now have a matrix\n",
    "print(net_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_i and h_j should exclude input from i and j from h_ij\n",
    "h_i = np.diagonal(old_weights) * mem # this obtains the input each neuron receives from itself\n",
    "h_i = h_i[:, np.newaxis]             # turn h_i into a column vector so we can subtract from hij appropriately\n",
    "print(h_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_j = old_weights * mem              # element-wise multiply each row of old-weights by mem\n",
    "np.fill_diagonal(h_j,0)\n",
    "print(h_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hij = net_inputs - h_i - h_j\n",
    "print(hij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_synaptic  = hij * mem\n",
    "print(post_synaptic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_synaptic = post_synaptic.T #equivalent to np.outer(hij,mem)\n",
    "print(pre_synaptic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = old_weights + (1./n)*(hebbian_term - pre_synaptic - post_synaptic)\n",
    "print(new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's verify that that algorithms works (I'm kinda shaky on my linear algebra). Let's verify it against an inefficient, but more transparent, for-loops implementation...\n",
    "\n",
    "Again, the formula for the Storkey rule:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e0b194a405a470e54aacef9e75ced89d02f60844)\n",
    "\n",
    "where\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0e9c85f5bbf569acdfc8dd7ab8cccb742a0f856)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storkey_rule_loops(patterns):\n",
    "    memories = [x.flatten() for x in patterns]\n",
    "    neurons  = len(memories[0])\n",
    "    memMat = np.zeros((neurons,neurons))\n",
    "    for mem in memories:\n",
    "        oldMemMat = memMat.copy()\n",
    "        for i in range(neurons):\n",
    "            for j in range(neurons):\n",
    "                hij = np.sum([ oldMemMat[i,k]*mem[k] for k in range(neurons) if k not in [i,j] ])\n",
    "                hji = np.sum([ oldMemMat[j,k]*mem[k] for k in range(neurons) if k not in [i,j] ])\n",
    "                memMat[i,j] += (1./neurons)*(mem[i]*mem[j] - mem[i]*hji - mem[j]*hij)\n",
    "    return memMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit storkey_rule(pattern=pats[1], old_weights=storkey_rule(pats[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit storkey_rule_loops(pats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the matrix math version -- which uses the optimized numpy data structures and operations -- is much, much faster than the for-loops verison. Now compare the weights of the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loop_weights = storkey_rule_loops(pats)\n",
    "print(loop_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_weights = storkey_rule(pats[1], old_weights=storkey_rule(pats[0]))\n",
    "print(matrix_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_weights == loop_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, they are the same!\n",
    "\n",
    "Now let's show that the weights work, i.e., that feeding the network a noisy version of a trained pattern recovers the trained pattern itself. We'll just flip a certain number of random pixels on each row of the trained pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisify(pattern, numb_flipped):\n",
    "\n",
    "    noisy_pattern = pattern.copy()\n",
    "\n",
    "    for idx, row in enumerate(noisy_pattern):\n",
    "        choices = np.random.choice(range(len(row)), numb_flipped)\n",
    "        noisy_pattern[idx,choices] = -noisy_pattern[idx,choices]\n",
    "        \n",
    "    return noisy_pattern\n",
    "\n",
    "noisy_test_pat = noisify(pattern=test_pat, numb_flipped=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy_test_pat, cmap='Greys', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with that, and use the weights to update it. We'll update the units asynchronously (one at a time), and keep track of the energy of the network every so often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow(pattern, weights, theta=0, steps = 1000):\n",
    "    \n",
    "    pattern_flat = pattern.flatten()\n",
    "\n",
    "    if isinstance(theta, numbers.Number):\n",
    "        thetas = np.zeros(len(pattern_flat)) + theta\n",
    "    \n",
    "    for step in range(steps):\n",
    "        unit = np.random.randint(low=0, high=(len(pattern_flat)-1))\n",
    "        unit_weights = weights[unit,:]\n",
    "        net_input = np.dot(unit_weights,pattern_flat)\n",
    "        pattern_flat[unit] = 1 if (net_input > thetas[unit]) else -1\n",
    "        \n",
    "        if (step % 200) == 0:\n",
    "            energy = -0.5*np.dot(np.dot(pattern_flat.T,weights),pattern_flat) + np.dot(thetas,pattern_flat)\n",
    "            print(\"Energy at step {:05d} is now {}\".format(step,energy))\n",
    "\n",
    "    evolved_pattern = np.reshape(a=pattern_flat, newshape=(pattern.shape[0],pattern.shape[1]))\n",
    "    \n",
    "    return evolved_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test_pat_evolved = flow(noisy_test_pat, matrix_weights)\n",
    "plt.imshow(noisy_test_pat_evolved, cmap='Greys', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_pat, cmap='Greys', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lo, the network settles on the trained pattern."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
