{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an efficient algorithm for the Storkey Learning Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1, -1, -1, -1,  1],\n",
       "        [-1,  1, -1,  1, -1],\n",
       "        [ 1,  1, -1, -1, -1],\n",
       "        [-1,  1,  1, -1, -1],\n",
       "        [-1,  1,  1,  1,  1]],\n",
       "\n",
       "       [[ 1, -1, -1,  1,  1],\n",
       "        [-1,  1, -1,  1, -1],\n",
       "        [-1,  1, -1,  1, -1],\n",
       "        [-1, -1,  1,  1,  1],\n",
       "        [ 1,  1,  1, -1, -1]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nunits = 5\n",
    "npats = 2\n",
    "pats = 2 * np.random.binomial(1, 0.5, [npats, nunits, nunits]) - 1\n",
    "pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noLD/AcYh787CEP60AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108a13358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pat = pats[1]\n",
    "\n",
    "plt.imshow(test_pat, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the weights. **Whereas before we used Hebb's rule, now let's use the Storkey Learning Rule**. This rule has a few nice advantages over Hebb's rule: it allows the network to learn more patterns, its basins of attraction (to the stored patterns) are larger, the distribution of basin sizes is more even, and the shapes of the basins are more round. The weights at time `v` are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e0b194a405a470e54aacef9e75ced89d02f60844)\n",
    "\n",
    "where\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0e9c85f5bbf569acdfc8dd7ab8cccb742a0f856)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `n` is the number of neurons and $\\epsilon$ is a bit (+1 or -1) of pattern `v`.\n",
    "\n",
    "The second term of the rule is basically the Hebbian rule. The third and fourth terms basically account for the net input to neurons j and i using the current weights.\n",
    "\n",
    "Here is the algorithm I settled on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storkey_rule(pattern, old_weights=None):\n",
    "    \"\"\"\n",
    "    pattern: 2-dimensional array\n",
    "    old_weights: square array of length pattern.shape[0]*pattern.shape[1]\n",
    "    \"\"\"\n",
    "    \n",
    "    mem = pattern.flatten()    \n",
    "    n = len(mem)\n",
    "    \n",
    "    if old_weights is None:\n",
    "        old_weights = np.zeros(shape=(n,n))\n",
    "\n",
    "    hebbian_term  = np.outer(mem,mem)\n",
    "    \n",
    "    net_inputs = old_weights.dot(mem)\n",
    "    net_inputs = np.tile(net_inputs, (n, 1)) # repeat the net_input vector n times along the rows \n",
    "                                             # so we now have a matrix\n",
    "    \n",
    "    # h_i and h_j should exclude input from i and j from h_ij\n",
    "    h_i = np.diagonal(old_weights) * mem # this obtains the input each neuron receives from itself\n",
    "    h_i = h_i[:, np.newaxis]             # turn h_i into a column vector so we can subtract from hij appropriately\n",
    "    \n",
    "    h_j = old_weights * mem              # element-wise multiply each row of old-weights by mem    \n",
    "    np.fill_diagonal(h_j,0)              # now replace the diagonal of h_j with 0's; the diagonal of h_j is the \n",
    "                                         # self-inputs, which are redundant with h_i\n",
    "    \n",
    "    hij = net_inputs - h_i - h_j\n",
    "    \n",
    "    post_synaptic  = hij * mem\n",
    "    pre_synaptic   = post_synaptic.T\n",
    "    #pre_synaptic   = hij.T * mem[:, np.newaxis]\n",
    "    \n",
    "    new_weights = old_weights + (1./n)*(hebbian_term - pre_synaptic - post_synaptic)\n",
    "    #new_weights = old_weights + (1./n)*(hebbian_term - 2*post_synaptic)\n",
    "    \n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [-1., -1., -1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(shape=(3,3)) - np.array(range(3))[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That algorithm is probably fairly opaque, owing to the matrix operations in it. Let's unpack it step-by-step. It's most illustrative to look at what happens when you are training a second pattern, because the weights of the first pattern are simply due to the hebbian term -- the other terms are just 0's. So, we'll start with `old_weights` from training the first test pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "old_weights = storkey_rule(pats[0])\n",
    "\n",
    "pattern = pats[1]\n",
    "\n",
    "mem = pattern.flatten()\n",
    "print(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "n = len(mem)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [-1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1\n",
      "  -1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]\n",
      " [ 1 -1 -1 -1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1  1  1\n",
      "   1]]\n"
     ]
    }
   ],
   "source": [
    "if old_weights is None:\n",
    "    old_weights = np.zeros(shape=(n,n))\n",
    "\n",
    "hebbian_term  = np.outer(mem,mem)\n",
    "print(hebbian_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "  0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "  0.12]\n"
     ]
    }
   ],
   "source": [
    "net_inputs = old_weights.dot(mem)\n",
    "print(net_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.12 -0.12  0.12  0.12  0.12  0.12 -0.12  0.12  0.12 -0.12 -0.12\n",
      "   0.12 -0.12  0.12 -0.12 -0.12  0.12 -0.12  0.12 -0.12  0.12  0.12  0.12\n",
      "   0.12]]\n"
     ]
    }
   ],
   "source": [
    "net_inputs = np.tile(net_inputs, (n, 1)) # repeat the net_input vector n times along the rows \n",
    "                                         # so we now have a matrix\n",
    "print(net_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [ 0.04]\n",
      " [-0.04]\n",
      " [ 0.04]\n",
      " [-0.04]\n",
      " [ 0.04]\n",
      " [ 0.04]\n",
      " [-0.04]\n",
      " [ 0.04]\n",
      " [ 0.04]\n",
      " [ 0.04]\n",
      " [ 0.04]\n",
      " [-0.04]\n",
      " [ 0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [ 0.04]\n",
      " [ 0.04]\n",
      " [ 0.04]]\n"
     ]
    }
   ],
   "source": [
    "# h_i and h_j should exclude input from i and j from h_ij\n",
    "h_i = np.diagonal(old_weights) * mem # this obtains the input each neuron receives from itself\n",
    "h_i = h_i[:, np.newaxis]             # turn h_i into a column vector so we can subtract from hij appropriately\n",
    "print(h_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.04  0.    0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [-0.04  0.04  0.    0.04  0.04  0.04  0.04  0.04  0.04 -0.04 -0.04  0.04\n",
      "  -0.04 -0.04 -0.04  0.04  0.04 -0.04 -0.04 -0.04 -0.04  0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [ 0.04 -0.04  0.04  0.   -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04  0.   -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04  0.   -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04  0.   -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [-0.04  0.04 -0.04  0.04  0.04  0.04  0.04  0.    0.04 -0.04 -0.04  0.04\n",
      "  -0.04 -0.04 -0.04  0.04  0.04 -0.04 -0.04 -0.04 -0.04  0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.    0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.    0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [-0.04  0.04 -0.04  0.04  0.04  0.04  0.04  0.04  0.04 -0.04  0.    0.04\n",
      "  -0.04 -0.04 -0.04  0.04  0.04 -0.04 -0.04 -0.04 -0.04  0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [-0.04  0.04 -0.04  0.04  0.04  0.04  0.04  0.04  0.04 -0.04 -0.04  0.\n",
      "  -0.04 -0.04 -0.04  0.04  0.04 -0.04 -0.04 -0.04 -0.04  0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.    0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [-0.04  0.04 -0.04  0.04  0.04  0.04  0.04  0.04  0.04 -0.04 -0.04  0.04\n",
      "  -0.04  0.   -0.04  0.04  0.04 -0.04 -0.04 -0.04 -0.04  0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.   -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [-0.04  0.04 -0.04  0.04  0.04  0.04  0.04  0.04  0.04 -0.04 -0.04  0.04\n",
      "  -0.04 -0.04 -0.04  0.    0.04 -0.04 -0.04 -0.04 -0.04  0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [-0.04  0.04 -0.04  0.04  0.04  0.04  0.04  0.04  0.04 -0.04 -0.04  0.04\n",
      "  -0.04 -0.04 -0.04  0.04  0.   -0.04 -0.04 -0.04 -0.04  0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.    0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [-0.04  0.04 -0.04  0.04  0.04  0.04  0.04  0.04  0.04 -0.04 -0.04  0.04\n",
      "  -0.04 -0.04 -0.04  0.04  0.04 -0.04  0.   -0.04 -0.04  0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.    0.04 -0.04  0.04  0.04\n",
      "   0.04]\n",
      " [-0.04  0.04 -0.04  0.04  0.04  0.04  0.04  0.04  0.04 -0.04 -0.04  0.04\n",
      "  -0.04 -0.04 -0.04  0.04  0.04 -0.04 -0.04 -0.04  0.    0.04 -0.04 -0.04\n",
      "  -0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04  0.    0.04  0.04\n",
      "   0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.    0.04\n",
      "   0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.\n",
      "   0.04]\n",
      " [ 0.04 -0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04 -0.04  0.04  0.04 -0.04\n",
      "   0.04  0.04  0.04 -0.04 -0.04  0.04  0.04  0.04  0.04 -0.04  0.04  0.04\n",
      "   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "h_j = old_weights * mem              # element-wise multiply each row of old-weights by mem\n",
    "np.fill_diagonal(h_j,0)\n",
    "print(h_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.04 -0.2  -0.12\n",
      "   0.04 -0.2   0.04 -0.12 -0.12  0.04 -0.2   0.04 -0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.12  0.16 -0.12  0.2   0.2   0.2   0.2  -0.04  0.2   0.12 -0.12 -0.04\n",
      "   0.12 -0.12  0.12 -0.04 -0.04  0.12 -0.12  0.12 -0.12  0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.2   0.12 -0.08  0.12  0.12  0.12  0.12 -0.12  0.12  0.2  -0.04 -0.12\n",
      "   0.2  -0.04  0.2  -0.12 -0.12  0.2  -0.04  0.2  -0.04  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.12  0.2  -0.12  0.16  0.2   0.2   0.2  -0.04  0.2   0.12 -0.12 -0.04\n",
      "   0.12 -0.12  0.12 -0.04 -0.04  0.12 -0.12  0.12 -0.12  0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.2  -0.12  0.2   0.16  0.2   0.2  -0.04  0.2   0.12 -0.12 -0.04\n",
      "   0.12 -0.12  0.12 -0.04 -0.04  0.12 -0.12  0.12 -0.12  0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.2  -0.12  0.2   0.2   0.16  0.2  -0.04  0.2   0.12 -0.12 -0.04\n",
      "   0.12 -0.12  0.12 -0.04 -0.04  0.12 -0.12  0.12 -0.12  0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.2  -0.12  0.2   0.2   0.2   0.16 -0.04  0.2   0.12 -0.12 -0.04\n",
      "   0.12 -0.12  0.12 -0.04 -0.04  0.12 -0.12  0.12 -0.12  0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.04 -0.12  0.04  0.04  0.04  0.04 -0.16  0.04  0.12 -0.12 -0.2\n",
      "   0.12 -0.12  0.12 -0.2  -0.2   0.12 -0.12  0.12 -0.12  0.04  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.2  -0.12  0.2   0.2   0.2   0.2  -0.04  0.16  0.12 -0.12 -0.04\n",
      "   0.12 -0.12  0.12 -0.04 -0.04  0.12 -0.12  0.12 -0.12  0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.04  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.08 -0.2  -0.12\n",
      "   0.04 -0.2   0.04 -0.12 -0.12  0.04 -0.2   0.04 -0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2   0.12 -0.04  0.12  0.12  0.12  0.12 -0.12  0.12  0.2  -0.08 -0.12\n",
      "   0.2  -0.04  0.2  -0.12 -0.12  0.2  -0.04  0.2  -0.04  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.12  0.04 -0.12  0.04  0.04  0.04  0.04 -0.2   0.04  0.12 -0.12 -0.16\n",
      "   0.12 -0.12  0.12 -0.2  -0.2   0.12 -0.12  0.12 -0.12  0.04  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.04  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.04 -0.2  -0.12\n",
      "   0.08 -0.2   0.04 -0.12 -0.12  0.04 -0.2   0.04 -0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2   0.12 -0.04  0.12  0.12  0.12  0.12 -0.12  0.12  0.2  -0.04 -0.12\n",
      "   0.2  -0.08  0.2  -0.12 -0.12  0.2  -0.04  0.2  -0.04  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.04  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.04 -0.2  -0.12\n",
      "   0.04 -0.2   0.08 -0.12 -0.12  0.04 -0.2   0.04 -0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.12  0.04 -0.12  0.04  0.04  0.04  0.04 -0.2   0.04  0.12 -0.12 -0.2\n",
      "   0.12 -0.12  0.12 -0.16 -0.2   0.12 -0.12  0.12 -0.12  0.04  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12  0.04 -0.12  0.04  0.04  0.04  0.04 -0.2   0.04  0.12 -0.12 -0.2\n",
      "   0.12 -0.12  0.12 -0.2  -0.16  0.12 -0.12  0.12 -0.12  0.04  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.04  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.04 -0.2  -0.12\n",
      "   0.04 -0.2   0.04 -0.12 -0.12  0.08 -0.2   0.04 -0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2   0.12 -0.04  0.12  0.12  0.12  0.12 -0.12  0.12  0.2  -0.04 -0.12\n",
      "   0.2  -0.04  0.2  -0.12 -0.12  0.2  -0.08  0.2  -0.04  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.04  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.04 -0.2  -0.12\n",
      "   0.04 -0.2   0.04 -0.12 -0.12  0.04 -0.2   0.08 -0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2   0.12 -0.04  0.12  0.12  0.12  0.12 -0.12  0.12  0.2  -0.04 -0.12\n",
      "   0.2  -0.04  0.2  -0.12 -0.12  0.2  -0.04  0.2  -0.08  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.12  0.2  -0.12  0.2   0.2   0.2   0.2  -0.04  0.2   0.12 -0.12 -0.04\n",
      "   0.12 -0.12  0.12 -0.04 -0.04  0.12 -0.12  0.12 -0.12  0.16  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.04  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.04 -0.2  -0.12\n",
      "   0.04 -0.2   0.04 -0.12 -0.12  0.04 -0.2   0.04 -0.2   0.12  0.08  0.04\n",
      "   0.04]\n",
      " [ 0.04  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.04 -0.2  -0.12\n",
      "   0.04 -0.2   0.04 -0.12 -0.12  0.04 -0.2   0.04 -0.2   0.12  0.04  0.08\n",
      "   0.04]\n",
      " [ 0.04  0.12 -0.2   0.12  0.12  0.12  0.12 -0.12  0.12  0.04 -0.2  -0.12\n",
      "   0.04 -0.2   0.04 -0.12 -0.12  0.04 -0.2   0.04 -0.2   0.12  0.04  0.04\n",
      "   0.08]]\n"
     ]
    }
   ],
   "source": [
    "hij = net_inputs - h_i - h_j\n",
    "print(hij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.04  0.2  -0.12\n",
      "   0.04  0.2   0.04 -0.12 -0.12  0.04  0.2   0.04  0.2  -0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.12 -0.16  0.12 -0.2  -0.2  -0.2  -0.2  -0.04 -0.2   0.12  0.12 -0.04\n",
      "   0.12  0.12  0.12 -0.04 -0.04  0.12  0.12  0.12  0.12 -0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.2  -0.12  0.08 -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.2   0.04 -0.12\n",
      "   0.2   0.04  0.2  -0.12 -0.12  0.2   0.04  0.2   0.04 -0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.12 -0.2   0.12 -0.16 -0.2  -0.2  -0.2  -0.04 -0.2   0.12  0.12 -0.04\n",
      "   0.12  0.12  0.12 -0.04 -0.04  0.12  0.12  0.12  0.12 -0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12 -0.2   0.12 -0.2  -0.16 -0.2  -0.2  -0.04 -0.2   0.12  0.12 -0.04\n",
      "   0.12  0.12  0.12 -0.04 -0.04  0.12  0.12  0.12  0.12 -0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12 -0.2   0.12 -0.2  -0.2  -0.16 -0.2  -0.04 -0.2   0.12  0.12 -0.04\n",
      "   0.12  0.12  0.12 -0.04 -0.04  0.12  0.12  0.12  0.12 -0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12 -0.2   0.12 -0.2  -0.2  -0.2  -0.16 -0.04 -0.2   0.12  0.12 -0.04\n",
      "   0.12  0.12  0.12 -0.04 -0.04  0.12  0.12  0.12  0.12 -0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12 -0.04  0.12 -0.04 -0.04 -0.04 -0.04 -0.16 -0.04  0.12  0.12 -0.2\n",
      "   0.12  0.12  0.12 -0.2  -0.2   0.12  0.12  0.12  0.12 -0.04  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12 -0.2   0.12 -0.2  -0.2  -0.2  -0.2  -0.04 -0.16  0.12  0.12 -0.04\n",
      "   0.12  0.12  0.12 -0.04 -0.04  0.12  0.12  0.12  0.12 -0.2   0.12  0.12\n",
      "   0.12]\n",
      " [ 0.04 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.08  0.2  -0.12\n",
      "   0.04  0.2   0.04 -0.12 -0.12  0.04  0.2   0.04  0.2  -0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2  -0.12  0.04 -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.2   0.08 -0.12\n",
      "   0.2   0.04  0.2  -0.12 -0.12  0.2   0.04  0.2   0.04 -0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.12 -0.04  0.12 -0.04 -0.04 -0.04 -0.04 -0.2  -0.04  0.12  0.12 -0.16\n",
      "   0.12  0.12  0.12 -0.2  -0.2   0.12  0.12  0.12  0.12 -0.04  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.04 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.04  0.2  -0.12\n",
      "   0.08  0.2   0.04 -0.12 -0.12  0.04  0.2   0.04  0.2  -0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2  -0.12  0.04 -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.2   0.04 -0.12\n",
      "   0.2   0.08  0.2  -0.12 -0.12  0.2   0.04  0.2   0.04 -0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.04 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.04  0.2  -0.12\n",
      "   0.04  0.2   0.08 -0.12 -0.12  0.04  0.2   0.04  0.2  -0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.12 -0.04  0.12 -0.04 -0.04 -0.04 -0.04 -0.2  -0.04  0.12  0.12 -0.2\n",
      "   0.12  0.12  0.12 -0.16 -0.2   0.12  0.12  0.12  0.12 -0.04  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.12 -0.04  0.12 -0.04 -0.04 -0.04 -0.04 -0.2  -0.04  0.12  0.12 -0.2\n",
      "   0.12  0.12  0.12 -0.2  -0.16  0.12  0.12  0.12  0.12 -0.04  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.04 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.04  0.2  -0.12\n",
      "   0.04  0.2   0.04 -0.12 -0.12  0.08  0.2   0.04  0.2  -0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2  -0.12  0.04 -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.2   0.04 -0.12\n",
      "   0.2   0.04  0.2  -0.12 -0.12  0.2   0.08  0.2   0.04 -0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.04 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.04  0.2  -0.12\n",
      "   0.04  0.2   0.04 -0.12 -0.12  0.04  0.2   0.08  0.2  -0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2  -0.12  0.04 -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.2   0.04 -0.12\n",
      "   0.2   0.04  0.2  -0.12 -0.12  0.2   0.04  0.2   0.08 -0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.12 -0.2   0.12 -0.2  -0.2  -0.2  -0.2  -0.04 -0.2   0.12  0.12 -0.04\n",
      "   0.12  0.12  0.12 -0.04 -0.04  0.12  0.12  0.12  0.12 -0.16  0.12  0.12\n",
      "   0.12]\n",
      " [ 0.04 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.04  0.2  -0.12\n",
      "   0.04  0.2   0.04 -0.12 -0.12  0.04  0.2   0.04  0.2  -0.12  0.08  0.04\n",
      "   0.04]\n",
      " [ 0.04 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.04  0.2  -0.12\n",
      "   0.04  0.2   0.04 -0.12 -0.12  0.04  0.2   0.04  0.2  -0.12  0.04  0.08\n",
      "   0.04]\n",
      " [ 0.04 -0.12  0.2  -0.12 -0.12 -0.12 -0.12 -0.12 -0.12  0.04  0.2  -0.12\n",
      "   0.04  0.2   0.04 -0.12 -0.12  0.04  0.2   0.04  0.2  -0.12  0.04  0.04\n",
      "   0.08]]\n"
     ]
    }
   ],
   "source": [
    "post_synaptic  = hij * mem\n",
    "print(post_synaptic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.04  0.2   0.12\n",
      "   0.04  0.2   0.04  0.12  0.12  0.04  0.2   0.04  0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [-0.12 -0.16 -0.12 -0.2  -0.2  -0.2  -0.2  -0.04 -0.2  -0.12 -0.12 -0.04\n",
      "  -0.12 -0.12 -0.12 -0.04 -0.04 -0.12 -0.12 -0.12 -0.12 -0.2  -0.12 -0.12\n",
      "  -0.12]\n",
      " [ 0.2   0.12  0.08  0.12  0.12  0.12  0.12  0.12  0.12  0.2   0.04  0.12\n",
      "   0.2   0.04  0.2   0.12  0.12  0.2   0.04  0.2   0.04  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [-0.12 -0.2  -0.12 -0.16 -0.2  -0.2  -0.2  -0.04 -0.2  -0.12 -0.12 -0.04\n",
      "  -0.12 -0.12 -0.12 -0.04 -0.04 -0.12 -0.12 -0.12 -0.12 -0.2  -0.12 -0.12\n",
      "  -0.12]\n",
      " [-0.12 -0.2  -0.12 -0.2  -0.16 -0.2  -0.2  -0.04 -0.2  -0.12 -0.12 -0.04\n",
      "  -0.12 -0.12 -0.12 -0.04 -0.04 -0.12 -0.12 -0.12 -0.12 -0.2  -0.12 -0.12\n",
      "  -0.12]\n",
      " [-0.12 -0.2  -0.12 -0.2  -0.2  -0.16 -0.2  -0.04 -0.2  -0.12 -0.12 -0.04\n",
      "  -0.12 -0.12 -0.12 -0.04 -0.04 -0.12 -0.12 -0.12 -0.12 -0.2  -0.12 -0.12\n",
      "  -0.12]\n",
      " [-0.12 -0.2  -0.12 -0.2  -0.2  -0.2  -0.16 -0.04 -0.2  -0.12 -0.12 -0.04\n",
      "  -0.12 -0.12 -0.12 -0.04 -0.04 -0.12 -0.12 -0.12 -0.12 -0.2  -0.12 -0.12\n",
      "  -0.12]\n",
      " [-0.12 -0.04 -0.12 -0.04 -0.04 -0.04 -0.04 -0.16 -0.04 -0.12 -0.12 -0.2\n",
      "  -0.12 -0.12 -0.12 -0.2  -0.2  -0.12 -0.12 -0.12 -0.12 -0.04 -0.12 -0.12\n",
      "  -0.12]\n",
      " [-0.12 -0.2  -0.12 -0.2  -0.2  -0.2  -0.2  -0.04 -0.16 -0.12 -0.12 -0.04\n",
      "  -0.12 -0.12 -0.12 -0.04 -0.04 -0.12 -0.12 -0.12 -0.12 -0.2  -0.12 -0.12\n",
      "  -0.12]\n",
      " [ 0.04  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.08  0.2   0.12\n",
      "   0.04  0.2   0.04  0.12  0.12  0.04  0.2   0.04  0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2   0.12  0.04  0.12  0.12  0.12  0.12  0.12  0.12  0.2   0.08  0.12\n",
      "   0.2   0.04  0.2   0.12  0.12  0.2   0.04  0.2   0.04  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [-0.12 -0.04 -0.12 -0.04 -0.04 -0.04 -0.04 -0.2  -0.04 -0.12 -0.12 -0.16\n",
      "  -0.12 -0.12 -0.12 -0.2  -0.2  -0.12 -0.12 -0.12 -0.12 -0.04 -0.12 -0.12\n",
      "  -0.12]\n",
      " [ 0.04  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.04  0.2   0.12\n",
      "   0.08  0.2   0.04  0.12  0.12  0.04  0.2   0.04  0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2   0.12  0.04  0.12  0.12  0.12  0.12  0.12  0.12  0.2   0.04  0.12\n",
      "   0.2   0.08  0.2   0.12  0.12  0.2   0.04  0.2   0.04  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.04  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.04  0.2   0.12\n",
      "   0.04  0.2   0.08  0.12  0.12  0.04  0.2   0.04  0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [-0.12 -0.04 -0.12 -0.04 -0.04 -0.04 -0.04 -0.2  -0.04 -0.12 -0.12 -0.2\n",
      "  -0.12 -0.12 -0.12 -0.16 -0.2  -0.12 -0.12 -0.12 -0.12 -0.04 -0.12 -0.12\n",
      "  -0.12]\n",
      " [-0.12 -0.04 -0.12 -0.04 -0.04 -0.04 -0.04 -0.2  -0.04 -0.12 -0.12 -0.2\n",
      "  -0.12 -0.12 -0.12 -0.2  -0.16 -0.12 -0.12 -0.12 -0.12 -0.04 -0.12 -0.12\n",
      "  -0.12]\n",
      " [ 0.04  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.04  0.2   0.12\n",
      "   0.04  0.2   0.04  0.12  0.12  0.08  0.2   0.04  0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2   0.12  0.04  0.12  0.12  0.12  0.12  0.12  0.12  0.2   0.04  0.12\n",
      "   0.2   0.04  0.2   0.12  0.12  0.2   0.08  0.2   0.04  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [ 0.04  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.04  0.2   0.12\n",
      "   0.04  0.2   0.04  0.12  0.12  0.04  0.2   0.08  0.2   0.12  0.04  0.04\n",
      "   0.04]\n",
      " [ 0.2   0.12  0.04  0.12  0.12  0.12  0.12  0.12  0.12  0.2   0.04  0.12\n",
      "   0.2   0.04  0.2   0.12  0.12  0.2   0.04  0.2   0.08  0.12  0.2   0.2\n",
      "   0.2 ]\n",
      " [-0.12 -0.2  -0.12 -0.2  -0.2  -0.2  -0.2  -0.04 -0.2  -0.12 -0.12 -0.04\n",
      "  -0.12 -0.12 -0.12 -0.04 -0.04 -0.12 -0.12 -0.12 -0.12 -0.16 -0.12 -0.12\n",
      "  -0.12]\n",
      " [ 0.04  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.04  0.2   0.12\n",
      "   0.04  0.2   0.04  0.12  0.12  0.04  0.2   0.04  0.2   0.12  0.08  0.04\n",
      "   0.04]\n",
      " [ 0.04  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.04  0.2   0.12\n",
      "   0.04  0.2   0.04  0.12  0.12  0.04  0.2   0.04  0.2   0.12  0.04  0.08\n",
      "   0.04]\n",
      " [ 0.04  0.12  0.2   0.12  0.12  0.12  0.12  0.12  0.12  0.04  0.2   0.12\n",
      "   0.04  0.2   0.04  0.12  0.12  0.04  0.2   0.04  0.2   0.12  0.04  0.04\n",
      "   0.08]]\n"
     ]
    }
   ],
   "source": [
    "pre_synaptic = post_synaptic.T #equivalent to np.outer(hij,mem)\n",
    "print(pre_synaptic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0736  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0768 -0.096   0.      0.0768 -0.096   0.0768  0.      0.      0.0768\n",
      "  -0.096   0.0768 -0.096   0.      0.0768  0.0768  0.0768]\n",
      " [ 0.      0.0928  0.      0.096   0.096   0.096   0.096  -0.0768  0.096\n",
      "   0.      0.     -0.0768  0.      0.      0.     -0.0768 -0.0768  0.      0.\n",
      "   0.      0.      0.096   0.      0.      0.    ]\n",
      " [-0.096   0.      0.0736  0.      0.      0.      0.      0.      0.\n",
      "  -0.096   0.0768  0.     -0.096   0.0768 -0.096   0.      0.     -0.096\n",
      "   0.0768 -0.096   0.0768  0.     -0.096  -0.096  -0.096 ]\n",
      " [ 0.      0.096   0.      0.0928  0.096   0.096   0.096  -0.0768  0.096\n",
      "   0.      0.     -0.0768  0.      0.      0.     -0.0768 -0.0768  0.      0.\n",
      "   0.      0.      0.096   0.      0.      0.    ]\n",
      " [ 0.      0.096   0.      0.096   0.0928  0.096   0.096  -0.0768  0.096\n",
      "   0.      0.     -0.0768  0.      0.      0.     -0.0768 -0.0768  0.      0.\n",
      "   0.      0.      0.096   0.      0.      0.    ]\n",
      " [ 0.      0.096   0.      0.096   0.096   0.0928  0.096  -0.0768  0.096\n",
      "   0.      0.     -0.0768  0.      0.      0.     -0.0768 -0.0768  0.      0.\n",
      "   0.      0.      0.096   0.      0.      0.    ]\n",
      " [ 0.      0.096   0.      0.096   0.096   0.096   0.0928 -0.0768  0.096\n",
      "   0.      0.     -0.0768  0.      0.      0.     -0.0768 -0.0768  0.      0.\n",
      "   0.      0.      0.096   0.      0.      0.    ]\n",
      " [ 0.     -0.0768  0.     -0.0768 -0.0768 -0.0768 -0.0768  0.0928 -0.0768\n",
      "   0.      0.      0.096   0.      0.      0.      0.096   0.096   0.      0.\n",
      "   0.      0.     -0.0768  0.      0.      0.    ]\n",
      " [ 0.      0.096   0.      0.096   0.096   0.096   0.096  -0.0768  0.0928\n",
      "   0.      0.     -0.0768  0.      0.      0.     -0.0768 -0.0768  0.      0.\n",
      "   0.      0.      0.096   0.      0.      0.    ]\n",
      " [ 0.0768  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0736 -0.096   0.      0.0768 -0.096   0.0768  0.      0.      0.0768\n",
      "  -0.096   0.0768 -0.096   0.      0.0768  0.0768  0.0768]\n",
      " [-0.096   0.      0.0768  0.      0.      0.      0.      0.      0.\n",
      "  -0.096   0.0736  0.     -0.096   0.0768 -0.096   0.      0.     -0.096\n",
      "   0.0768 -0.096   0.0768  0.     -0.096  -0.096  -0.096 ]\n",
      " [ 0.     -0.0768  0.     -0.0768 -0.0768 -0.0768 -0.0768  0.096  -0.0768\n",
      "   0.      0.      0.0928  0.      0.      0.      0.096   0.096   0.      0.\n",
      "   0.      0.     -0.0768  0.      0.      0.    ]\n",
      " [ 0.0768  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0768 -0.096   0.      0.0736 -0.096   0.0768  0.      0.      0.0768\n",
      "  -0.096   0.0768 -0.096   0.      0.0768  0.0768  0.0768]\n",
      " [-0.096   0.      0.0768  0.      0.      0.      0.      0.      0.\n",
      "  -0.096   0.0768  0.     -0.096   0.0736 -0.096   0.      0.     -0.096\n",
      "   0.0768 -0.096   0.0768  0.     -0.096  -0.096  -0.096 ]\n",
      " [ 0.0768  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0768 -0.096   0.      0.0768 -0.096   0.0736  0.      0.      0.0768\n",
      "  -0.096   0.0768 -0.096   0.      0.0768  0.0768  0.0768]\n",
      " [ 0.     -0.0768  0.     -0.0768 -0.0768 -0.0768 -0.0768  0.096  -0.0768\n",
      "   0.      0.      0.096   0.      0.      0.      0.0928  0.096   0.      0.\n",
      "   0.      0.     -0.0768  0.      0.      0.    ]\n",
      " [ 0.     -0.0768  0.     -0.0768 -0.0768 -0.0768 -0.0768  0.096  -0.0768\n",
      "   0.      0.      0.096   0.      0.      0.      0.096   0.0928  0.      0.\n",
      "   0.      0.     -0.0768  0.      0.      0.    ]\n",
      " [ 0.0768  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0768 -0.096   0.      0.0768 -0.096   0.0768  0.      0.      0.0736\n",
      "  -0.096   0.0768 -0.096   0.      0.0768  0.0768  0.0768]\n",
      " [-0.096   0.      0.0768  0.      0.      0.      0.      0.      0.\n",
      "  -0.096   0.0768  0.     -0.096   0.0768 -0.096   0.      0.     -0.096\n",
      "   0.0736 -0.096   0.0768  0.     -0.096  -0.096  -0.096 ]\n",
      " [ 0.0768  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0768 -0.096   0.      0.0768 -0.096   0.0768  0.      0.      0.0768\n",
      "  -0.096   0.0736 -0.096   0.      0.0768  0.0768  0.0768]\n",
      " [-0.096   0.      0.0768  0.      0.      0.      0.      0.      0.\n",
      "  -0.096   0.0768  0.     -0.096   0.0768 -0.096   0.      0.     -0.096\n",
      "   0.0768 -0.096   0.0736  0.     -0.096  -0.096  -0.096 ]\n",
      " [ 0.      0.096   0.      0.096   0.096   0.096   0.096  -0.0768  0.096\n",
      "   0.      0.     -0.0768  0.      0.      0.     -0.0768 -0.0768  0.      0.\n",
      "   0.      0.      0.0928  0.      0.      0.    ]\n",
      " [ 0.0768  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0768 -0.096   0.      0.0768 -0.096   0.0768  0.      0.      0.0768\n",
      "  -0.096   0.0768 -0.096   0.      0.0736  0.0768  0.0768]\n",
      " [ 0.0768  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0768 -0.096   0.      0.0768 -0.096   0.0768  0.      0.      0.0768\n",
      "  -0.096   0.0768 -0.096   0.      0.0768  0.0736  0.0768]\n",
      " [ 0.0768  0.     -0.096   0.      0.      0.      0.      0.      0.\n",
      "   0.0768 -0.096   0.      0.0768 -0.096   0.0768  0.      0.      0.0768\n",
      "  -0.096   0.0768 -0.096   0.      0.0768  0.0768  0.0736]]\n"
     ]
    }
   ],
   "source": [
    "new_weights = old_weights + (1./n)*(hebbian_term - pre_synaptic - post_synaptic)\n",
    "print(new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's verify that that algorithms works (I'm kinda shaky on my linear algebra). Let's verify it against an inefficient, but more transparent, for-loops implementation...\n",
    "\n",
    "Again, the formula for the Storkey rule:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e0b194a405a470e54aacef9e75ced89d02f60844)\n",
    "\n",
    "where\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0e9c85f5bbf569acdfc8dd7ab8cccb742a0f856)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storkey_rule_loops(patterns):\n",
    "    memories = [x.flatten() for x in patterns]\n",
    "    neurons  = len(memories[0])\n",
    "    memMat = np.zeros((neurons,neurons))\n",
    "    for mem in memories:\n",
    "        oldMemMat = memMat.copy()\n",
    "        for i in range(neurons):\n",
    "            for j in range(neurons):\n",
    "                hij = np.sum([ oldMemMat[i,k]*mem[k] for k in range(neurons) if k not in [i,j] ])\n",
    "                hji = np.sum([ oldMemMat[j,k]*mem[k] for k in range(neurons) if k not in [i,j] ])\n",
    "                memMat[i,j] += (1./neurons)*(mem[i]*mem[j] - mem[i]*hji - mem[j]*hij)\n",
    "    return memMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 11.37 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 146 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit storkey_rule(pattern=pats[1], old_weights=storkey_rule(pats[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 98.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit storkey_rule_loops(pats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the matrix math version -- which uses the optimized numpy data structures and operations -- is much, much faster than the for-loops verison. Now compare the weights of the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0992  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.1024  0.      0.      0.1024  0.      0.     -0.1024  0.      0.1024\n",
      "   0.1024  0.1024  0.      0.     -0.1024 -0.1024]\n",
      " [ 0.      0.0672  0.0704  0.     -0.0704  0.0704 -0.0704  0.0704 -0.0704\n",
      "   0.0704  0.     -0.0704  0.0704  0.      0.0704  0.0704  0.     -0.0704\n",
      "   0.      0.      0.     -0.0704 -0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0672  0.     -0.0704  0.0704 -0.0704  0.0704 -0.0704\n",
      "   0.0704  0.     -0.0704  0.0704  0.      0.0704  0.0704  0.     -0.0704\n",
      "   0.      0.      0.     -0.0704 -0.0704  0.      0.    ]\n",
      " [ 0.1024  0.      0.      0.0992  0.      0.      0.      0.      0.      0.\n",
      "  -0.1024  0.      0.      0.1024  0.      0.     -0.1024  0.      0.1024\n",
      "   0.1024  0.1024  0.      0.     -0.1024 -0.1024]\n",
      " [ 0.     -0.0704 -0.0704  0.      0.0672 -0.0704  0.0704 -0.0704  0.0704\n",
      "  -0.0704  0.      0.0704 -0.0704  0.     -0.0704 -0.0704  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.0704  0.0672 -0.0704  0.0704 -0.0704\n",
      "   0.0704  0.     -0.0704  0.0704  0.      0.0704  0.0704  0.     -0.0704\n",
      "   0.      0.      0.     -0.0704 -0.0704  0.      0.    ]\n",
      " [ 0.     -0.0704 -0.0704  0.      0.0704 -0.0704  0.0672 -0.0704  0.0704\n",
      "  -0.0704  0.      0.0704 -0.0704  0.     -0.0704 -0.0704  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.0704  0.0704 -0.0704  0.0672 -0.0704\n",
      "   0.0704  0.     -0.0704  0.0704  0.      0.0704  0.0704  0.     -0.0704\n",
      "   0.      0.      0.     -0.0704 -0.0704  0.      0.    ]\n",
      " [ 0.     -0.0704 -0.0704  0.      0.0704 -0.0704  0.0704 -0.0704  0.0672\n",
      "  -0.0704  0.      0.0704 -0.0704  0.     -0.0704 -0.0704  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.0704  0.0704 -0.0704  0.0704 -0.0704\n",
      "   0.0672  0.     -0.0704  0.0704  0.      0.0704  0.0704  0.     -0.0704\n",
      "   0.      0.      0.     -0.0704 -0.0704  0.      0.    ]\n",
      " [-0.1024  0.      0.     -0.1024  0.      0.      0.      0.      0.      0.\n",
      "   0.0992  0.      0.     -0.1024  0.      0.      0.1024  0.     -0.1024\n",
      "  -0.1024 -0.1024  0.      0.      0.1024  0.1024]\n",
      " [ 0.     -0.0704 -0.0704  0.      0.0704 -0.0704  0.0704 -0.0704  0.0704\n",
      "  -0.0704  0.      0.0672 -0.0704  0.     -0.0704 -0.0704  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.0704  0.0704 -0.0704  0.0704 -0.0704\n",
      "   0.0704  0.     -0.0704  0.0672  0.      0.0704  0.0704  0.     -0.0704\n",
      "   0.      0.      0.     -0.0704 -0.0704  0.      0.    ]\n",
      " [ 0.1024  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.1024  0.      0.      0.0992  0.      0.     -0.1024  0.      0.1024\n",
      "   0.1024  0.1024  0.      0.     -0.1024 -0.1024]\n",
      " [ 0.      0.0704  0.0704  0.     -0.0704  0.0704 -0.0704  0.0704 -0.0704\n",
      "   0.0704  0.     -0.0704  0.0704  0.      0.0672  0.0704  0.     -0.0704\n",
      "   0.      0.      0.     -0.0704 -0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.0704  0.0704 -0.0704  0.0704 -0.0704\n",
      "   0.0704  0.     -0.0704  0.0704  0.      0.0704  0.0672  0.     -0.0704\n",
      "   0.      0.      0.     -0.0704 -0.0704  0.      0.    ]\n",
      " [-0.1024  0.      0.     -0.1024  0.      0.      0.      0.      0.      0.\n",
      "   0.1024  0.      0.     -0.1024  0.      0.      0.0992  0.     -0.1024\n",
      "  -0.1024 -0.1024  0.      0.      0.1024  0.1024]\n",
      " [ 0.     -0.0704 -0.0704  0.      0.0704 -0.0704  0.0704 -0.0704  0.0704\n",
      "  -0.0704  0.      0.0704 -0.0704  0.     -0.0704 -0.0704  0.      0.0672\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.1024  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.1024  0.      0.      0.1024  0.      0.     -0.1024  0.      0.0992\n",
      "   0.1024  0.1024  0.      0.     -0.1024 -0.1024]\n",
      " [ 0.1024  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.1024  0.      0.      0.1024  0.      0.     -0.1024  0.      0.1024\n",
      "   0.0992  0.1024  0.      0.     -0.1024 -0.1024]\n",
      " [ 0.1024  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.1024  0.      0.      0.1024  0.      0.     -0.1024  0.      0.1024\n",
      "   0.1024  0.0992  0.      0.     -0.1024 -0.1024]\n",
      " [ 0.     -0.0704 -0.0704  0.      0.0704 -0.0704  0.0704 -0.0704  0.0704\n",
      "  -0.0704  0.      0.0704 -0.0704  0.     -0.0704 -0.0704  0.      0.0704\n",
      "   0.      0.      0.      0.0672  0.0704  0.      0.    ]\n",
      " [ 0.     -0.0704 -0.0704  0.      0.0704 -0.0704  0.0704 -0.0704  0.0704\n",
      "  -0.0704  0.      0.0704 -0.0704  0.     -0.0704 -0.0704  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0672  0.      0.    ]\n",
      " [-0.1024  0.      0.     -0.1024  0.      0.      0.      0.      0.      0.\n",
      "   0.1024  0.      0.     -0.1024  0.      0.      0.1024  0.     -0.1024\n",
      "  -0.1024 -0.1024  0.      0.      0.0992  0.1024]\n",
      " [-0.1024  0.      0.     -0.1024  0.      0.      0.      0.      0.      0.\n",
      "   0.1024  0.      0.     -0.1024  0.      0.      0.1024  0.     -0.1024\n",
      "  -0.1024 -0.1024  0.      0.      0.1024  0.0992]]\n"
     ]
    }
   ],
   "source": [
    "loop_weights = storkey_rule_loops(pats)\n",
    "print(loop_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0992  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.0704  0.      0.      0.1024  0.      0.     -0.0704  0.      0.1024\n",
      "   0.1024  0.1024  0.      0.     -0.0704 -0.0704]\n",
      " [ 0.      0.0672  0.0704  0.     -0.1024  0.0704 -0.1024  0.0704 -0.1024\n",
      "   0.0704  0.     -0.1024  0.0704  0.      0.0704  0.0704  0.     -0.1024\n",
      "   0.      0.      0.     -0.1024 -0.1024  0.      0.    ]\n",
      " [ 0.      0.0704  0.0672  0.     -0.1024  0.0704 -0.1024  0.0704 -0.1024\n",
      "   0.0704  0.     -0.1024  0.0704  0.      0.0704  0.0704  0.     -0.1024\n",
      "   0.      0.      0.     -0.1024 -0.1024  0.      0.    ]\n",
      " [ 0.1024  0.      0.      0.0992  0.      0.      0.      0.      0.      0.\n",
      "  -0.0704  0.      0.      0.1024  0.      0.     -0.0704  0.      0.1024\n",
      "   0.1024  0.1024  0.      0.     -0.0704 -0.0704]\n",
      " [ 0.     -0.1024 -0.1024  0.      0.0672 -0.1024  0.0704 -0.1024  0.0704\n",
      "  -0.1024  0.      0.0704 -0.1024  0.     -0.1024 -0.1024  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.1024  0.0672 -0.1024  0.0704 -0.1024\n",
      "   0.0704  0.     -0.1024  0.0704  0.      0.0704  0.0704  0.     -0.1024\n",
      "   0.      0.      0.     -0.1024 -0.1024  0.      0.    ]\n",
      " [ 0.     -0.1024 -0.1024  0.      0.0704 -0.1024  0.0672 -0.1024  0.0704\n",
      "  -0.1024  0.      0.0704 -0.1024  0.     -0.1024 -0.1024  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.1024  0.0704 -0.1024  0.0672 -0.1024\n",
      "   0.0704  0.     -0.1024  0.0704  0.      0.0704  0.0704  0.     -0.1024\n",
      "   0.      0.      0.     -0.1024 -0.1024  0.      0.    ]\n",
      " [ 0.     -0.1024 -0.1024  0.      0.0704 -0.1024  0.0704 -0.1024  0.0672\n",
      "  -0.1024  0.      0.0704 -0.1024  0.     -0.1024 -0.1024  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.1024  0.0704 -0.1024  0.0704 -0.1024\n",
      "   0.0672  0.     -0.1024  0.0704  0.      0.0704  0.0704  0.     -0.1024\n",
      "   0.      0.      0.     -0.1024 -0.1024  0.      0.    ]\n",
      " [-0.0704  0.      0.     -0.0704  0.      0.      0.      0.      0.      0.\n",
      "   0.0992  0.      0.     -0.0704  0.      0.      0.1024  0.     -0.0704\n",
      "  -0.0704 -0.0704  0.      0.      0.1024  0.1024]\n",
      " [ 0.     -0.1024 -0.1024  0.      0.0704 -0.1024  0.0704 -0.1024  0.0704\n",
      "  -0.1024  0.      0.0672 -0.1024  0.     -0.1024 -0.1024  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.1024  0.0704 -0.1024  0.0704 -0.1024\n",
      "   0.0704  0.     -0.1024  0.0672  0.      0.0704  0.0704  0.     -0.1024\n",
      "   0.      0.      0.     -0.1024 -0.1024  0.      0.    ]\n",
      " [ 0.1024  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.0704  0.      0.      0.0992  0.      0.     -0.0704  0.      0.1024\n",
      "   0.1024  0.1024  0.      0.     -0.0704 -0.0704]\n",
      " [ 0.      0.0704  0.0704  0.     -0.1024  0.0704 -0.1024  0.0704 -0.1024\n",
      "   0.0704  0.     -0.1024  0.0704  0.      0.0672  0.0704  0.     -0.1024\n",
      "   0.      0.      0.     -0.1024 -0.1024  0.      0.    ]\n",
      " [ 0.      0.0704  0.0704  0.     -0.1024  0.0704 -0.1024  0.0704 -0.1024\n",
      "   0.0704  0.     -0.1024  0.0704  0.      0.0704  0.0672  0.     -0.1024\n",
      "   0.      0.      0.     -0.1024 -0.1024  0.      0.    ]\n",
      " [-0.0704  0.      0.     -0.0704  0.      0.      0.      0.      0.      0.\n",
      "   0.1024  0.      0.     -0.0704  0.      0.      0.0992  0.     -0.0704\n",
      "  -0.0704 -0.0704  0.      0.      0.1024  0.1024]\n",
      " [ 0.     -0.1024 -0.1024  0.      0.0704 -0.1024  0.0704 -0.1024  0.0704\n",
      "  -0.1024  0.      0.0704 -0.1024  0.     -0.1024 -0.1024  0.      0.0672\n",
      "   0.      0.      0.      0.0704  0.0704  0.      0.    ]\n",
      " [ 0.1024  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.0704  0.      0.      0.1024  0.      0.     -0.0704  0.      0.0992\n",
      "   0.1024  0.1024  0.      0.     -0.0704 -0.0704]\n",
      " [ 0.1024  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.0704  0.      0.      0.1024  0.      0.     -0.0704  0.      0.1024\n",
      "   0.0992  0.1024  0.      0.     -0.0704 -0.0704]\n",
      " [ 0.1024  0.      0.      0.1024  0.      0.      0.      0.      0.      0.\n",
      "  -0.0704  0.      0.      0.1024  0.      0.     -0.0704  0.      0.1024\n",
      "   0.1024  0.0992  0.      0.     -0.0704 -0.0704]\n",
      " [ 0.     -0.1024 -0.1024  0.      0.0704 -0.1024  0.0704 -0.1024  0.0704\n",
      "  -0.1024  0.      0.0704 -0.1024  0.     -0.1024 -0.1024  0.      0.0704\n",
      "   0.      0.      0.      0.0672  0.0704  0.      0.    ]\n",
      " [ 0.     -0.1024 -0.1024  0.      0.0704 -0.1024  0.0704 -0.1024  0.0704\n",
      "  -0.1024  0.      0.0704 -0.1024  0.     -0.1024 -0.1024  0.      0.0704\n",
      "   0.      0.      0.      0.0704  0.0672  0.      0.    ]\n",
      " [-0.0704  0.      0.     -0.0704  0.      0.      0.      0.      0.      0.\n",
      "   0.1024  0.      0.     -0.0704  0.      0.      0.1024  0.     -0.0704\n",
      "  -0.0704 -0.0704  0.      0.      0.0992  0.1024]\n",
      " [-0.0704  0.      0.     -0.0704  0.      0.      0.      0.      0.      0.\n",
      "   0.1024  0.      0.     -0.0704  0.      0.      0.1024  0.     -0.0704\n",
      "  -0.0704 -0.0704  0.      0.      0.1024  0.0992]]\n"
     ]
    }
   ],
   "source": [
    "matrix_weights = storkey_rule(pats[1], old_weights=storkey_rule(pats[0]))\n",
    "print(matrix_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True,  True,  True, False, False],\n",
       "       [ True, False, False,  True,  True, False,  True, False,  True,\n",
       "        False,  True,  True, False,  True, False, False,  True,  True,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True, False, False,  True,  True, False,  True, False,  True,\n",
       "        False,  True,  True, False,  True, False, False,  True,  True,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True, False, False,  True,  True, False,  True, False,  True,\n",
       "        False,  True,  True, False,  True, False, False,  True,  True,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        False, False, False,  True,  True,  True,  True],\n",
       "       [ True, False, False,  True,  True, False,  True, False,  True,\n",
       "        False,  True,  True, False,  True, False, False,  True,  True,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True],\n",
       "       [False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        False, False, False,  True,  True,  True,  True],\n",
       "       [ True, False, False,  True,  True, False,  True, False,  True,\n",
       "        False,  True,  True, False,  True, False, False,  True, False,\n",
       "         True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True,  True,  True, False, False],\n",
       "       [ True, False, False,  True, False, False, False, False, False,\n",
       "        False,  True, False, False,  True, False, False,  True,  True,\n",
       "         True,  True,  True, False,  True,  True,  True],\n",
       "       [ True, False, False,  True, False, False, False, False, False,\n",
       "        False,  True, False, False,  True, False, False,  True,  True,\n",
       "         True,  True,  True,  True, False,  True,  True],\n",
       "       [False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        False, False, False,  True,  True,  True,  True],\n",
       "       [False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        False, False, False,  True,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_weights == loop_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, they are the same!\n",
    "\n",
    "Now let's show that the weights work, i.e., that feeding the network a noisy version of a trained pattern recovers the trained pattern itself. We'll just flip a certain number of random pixels on each row of the trained pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisify(pattern, numb_flipped):\n",
    "\n",
    "    noisy_pattern = pattern.copy()\n",
    "\n",
    "    for idx, row in enumerate(noisy_pattern):\n",
    "        choices = np.random.choice(range(len(row)), numb_flipped)\n",
    "        noisy_pattern[idx,choices] = -noisy_pattern[idx,choices]\n",
    "        \n",
    "    return noisy_pattern\n",
    "\n",
    "noisy_test_pat = noisify(pattern=test_pat, numb_flipped=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy_test_pat, cmap='Greys', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with that, and use the weights to update it. We'll update the units asynchronously (one at a time), and keep track of the energy of the network every so often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flow(pattern, weights, theta=0, steps = 1000):\n",
    "    \n",
    "    pattern_flat = pattern.flatten()\n",
    "\n",
    "    if isinstance(theta, numbers.Number):\n",
    "        thetas = np.zeros(len(pattern_flat)) + theta\n",
    "    \n",
    "    for step in range(steps):\n",
    "        unit = np.random.randint(low=0, high=(len(pattern_flat)-1))\n",
    "        unit_weights = weights[unit,:]\n",
    "        net_input = np.dot(unit_weights,pattern_flat)\n",
    "        pattern_flat[unit] = 1 if (net_input > thetas[unit]) else -1\n",
    "        \n",
    "        if (step % 200) == 0:\n",
    "            energy = -0.5*np.dot(np.dot(pattern_flat.T,weights),pattern_flat) + np.dot(thetas,pattern_flat)\n",
    "            print(\"Energy at step {:05d} is now {}\".format(step,energy))\n",
    "\n",
    "    evolved_pattern = np.reshape(a=pattern_flat, newshape=(pattern.shape[0],pattern.shape[1]))\n",
    "    \n",
    "    return evolved_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test_pat_evolved = flow(noisy_test_pat, matrix_weights)\n",
    "plt.imshow(noisy_test_pat_evolved, cmap='Greys', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_pat, cmap='Greys', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lo, the network settles on the trained pattern."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
