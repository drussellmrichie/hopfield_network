{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Hopfield network of pattern recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopfield networks are a kind of recurrent neural network that model auto-associative memory: the ability to recall a memory from just a partial piece of that memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "# from skimage import img_as_ubyte\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in a meme. I'm partial to ['Deal with it'](https://a1.memecaptain.com/src_thumbs/22990.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal = 2 * np.random.binomial(1,.5,size=(5,5)) - 1\n",
    "#deal = imread('obama.png', mode=\"L\")\n",
    "deal = imread('small-deal-with-it-with-text.jpg', mode=\"L\")\n",
    "print(deal.shape)\n",
    "deal = deal.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(deal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert this to a 1 bit image, I convert everything darker than some threshold to black (1), and everything else to white (-1). Experimenting a bit with the particular image of the 'deal with it meme' that I have, a threshold of 80 seemed to work reasonably. The resulting image is still a bit rough around the edges, but it's recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvw_threshold = 80\n",
    "\n",
    "deal[deal <= bvw_threshold] = -1\n",
    "deal[deal >  bvw_threshold] = 1\n",
    "deal = -deal\n",
    "deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(deal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(deal, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the weights. **Whereas before we used Hebb's rule, now let's use the Storkey Learning Rule**. This rule has a few nice advantages over Hebb's rule: it allows the network to learn more patterns (the 'capacity is `n/sqrt(2*ln(n))` where `n` is the number of neurons in the network), its basins of attraction (to the stored patterns) are larger, the distribution of basin sizes is more even, and the shapes of the basins are more round. The weights at time `v` are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e0b194a405a470e54aacef9e75ced89d02f60844)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c0e9c85f5bbf569acdfc8dd7ab8cccb742a0f856)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `n` is the number of neurons and $\\epsilon$ is a bit (+1 or -1) of the pattern being trained at time `v`.\n",
    "\n",
    "The second term of the rule is basically the Hebbian rule. The third and fourth terms basically account for the net input to neurons j and i using the current weights.\n",
    "\n",
    "**To see the development/testing of the below implementation of the Storkey rule, see 'Hopfield Network of memes--Storkey Learning Rule development'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storkey_rule(pattern, old_weights=None):\n",
    "    \"\"\"\n",
    "    pattern: 2-dimensional array\n",
    "    old_weights: square array of length pattern.shape[0]*pattern.shape[1]\n",
    "    \"\"\"\n",
    "    \n",
    "    mem = pattern.flatten()    \n",
    "    n = len(mem)\n",
    "    \n",
    "    if old_weights is None:\n",
    "        old_weights = np.zeros(shape=(n,n))\n",
    "\n",
    "    hebbian_term  = np.outer(mem,mem)\n",
    "    \n",
    "    net_inputs = old_weights.dot(mem)\n",
    "    net_inputs = np.tile(net_inputs, (n, 1)) # repeat the net_input vector n times along the rows \n",
    "                                             # so we now have a matrix\n",
    "    \n",
    "    # h_i and h_j should exclude input from i and j from h_ij\n",
    "    h_i = np.diagonal(old_weights) * mem # this obtains the input each neuron receives from itself\n",
    "    h_i = h_i[:, np.newaxis]             # turn h_i into a column vector so we can subtract from hij appropriately\n",
    "    \n",
    "    h_j = old_weights * mem              # element-wise multiply each row of old-weights by mem    \n",
    "    np.fill_diagonal(h_j,0)              # now replace the diagonal of h_j with 0's; the diagonal of h_j is the \n",
    "                                         # self-inputs, which are redundant with h_i; np.fill_diagonal modifies inplace\n",
    "    \n",
    "    hij = net_inputs - h_i - h_j\n",
    "    \n",
    "    post_synaptic  = hij * mem\n",
    "    #pre_synaptic = post_synaptic.T\n",
    "    pre_synaptic   = hij.T * mem[:, np.newaxis]\n",
    "        \n",
    "    new_weights = old_weights + (1./n)*(hebbian_term - pre_synaptic - post_synaptic)\n",
    "    \n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This next cell can take a little while if the image is large. For an image of size 128x128, it takes a minute or two.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deal_weights = storkey_rule(deal, old_weights=None)\n",
    "deal_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start with a noisy version of the image. We'll just flip a certain number of random pixels on each row of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisify(pattern, numb_flipped=30):\n",
    "\n",
    "    noisy_pattern = pattern.copy()\n",
    "\n",
    "    for idx, row in enumerate(noisy_pattern):\n",
    "        choices = np.random.choice(range(len(row)), numb_flipped)\n",
    "        noisy_pattern[idx,choices] = -noisy_pattern[idx,choices]\n",
    "        \n",
    "    return noisy_pattern\n",
    "\n",
    "noisy_deal = noisify(pattern=deal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy_deal, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with that, and use the weights to update it. We'll update the units asynchronously (one at a time), and keep track of the energy of the network every so often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow(pattern, weights, theta=0, steps = 50000):\n",
    "    \n",
    "    pattern_flat = pattern.flatten()\n",
    "\n",
    "    if isinstance(theta, numbers.Number):\n",
    "        thetas = np.zeros(len(pattern_flat)) + theta\n",
    "    \n",
    "    for step in range(steps):\n",
    "        unit = np.random.randint(low=0, high=(len(pattern_flat)-1))\n",
    "        unit_weights = weights[unit,:]\n",
    "        net_input = np.dot(unit_weights,pattern_flat)\n",
    "        pattern_flat[unit] = 1 if (net_input > thetas[unit]) else -1        \n",
    "        #pattern_flat[unit] = np.sign(net_input)\n",
    "        \n",
    "        if (step % 10000) == 0:\n",
    "            energy = -0.5*np.dot(np.dot(pattern_flat.T,weights),pattern_flat) + np.dot(thetas,pattern_flat)\n",
    "            print(\"Energy at step {:05d} is now {}\".format(step,energy))\n",
    "\n",
    "    evolved_pattern = np.reshape(a=pattern_flat, newshape=(pattern.shape[0],pattern.shape[1]))\n",
    "    \n",
    "    return evolved_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 50000\n",
    "theta = 0\n",
    "\n",
    "noisy_deal_evolved = flow(noisy_deal, deal_weights, theta = theta, steps = steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy_deal_evolved, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network on a second pattern\n",
    "\n",
    "The cooler thing about the Hopfield networks is that they can encode multiple patterns (to a limit depending on the training regimen, and the number of units). So let's try another maymay.\n",
    "\n",
    "I got the next meme from [here](https://68.media.tumblr.com/avatar_0f24a9a67d83_128.png), and then tweaked its levels in Mac's preview so that it'd translate nicely to a 1 bit (black or white) image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# woah = imread('woah.png', mode=\"L\")\n",
    "woah = imageio.imread('aang.jpg')[:,:,0]\n",
    "woah = woah.astype(int)\n",
    "woah[woah >= 1] = 1\n",
    "woah[woah < 1] = -1\n",
    "woah = -woah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(woah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(woah, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. So now we make some weights for this image. The takes a little bit longer than the Hebbian learning rule when it is dealing with previous, nonzero weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_weights = storkey_rule(woah, old_weights=deal_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_woah = noisify(pattern=woah, numb_flipped=15)\n",
    "        \n",
    "plt.imshow(noisy_woah, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_woah = flow(noisy_woah, average_weights, theta = theta, steps = steps)\n",
    "\n",
    "plt.imshow(recovered_woah, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's doublecheck that the average weights also still work for the 'deal with it' image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deal_recovered = flow(noisy_deal, average_weights, theta = theta, steps = steps)\n",
    "\n",
    "plt.imshow(deal_recovered, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet. So *now* we can try something like feeding it a pattern that is halfway between the two patterns -- it should eventually settle into one of them! Who has greater meme strength!??!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deal_with_neil = (woah + deal) / 2\n",
    "print(np.unique(deal_with_neil))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could force those 0 values to -1 or 1, but that biases the pattern towards deal and neil, respectively (at least, testing suggested this -- I think because Aang has more black pixels and Deal has more white pixels). So, I'll leave them in. I *could* probably solve this by randomly setting 0's to 1 or -1, but naw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal_with_neil[deal_with_neil == 0] = -1\n",
    "#np.unique(deal_with_neil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(deal_with_neil, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_deal_with_neil = flow(deal_with_neil, average_weights, theta = theta, steps = steps)\n",
    "\n",
    "plt.imshow(recovered_deal_with_neil, cmap='Greys', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Assuming the cells/pixels of 0 were unaltered*, if you run that a few times, you'll notice that sometimes it settles on Neil, and sometimes it settles on Deal!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spurious patterns\n",
    "\n",
    "Hopfield networks can also settle onto 'spurious patterns' (patterns that the network wasn't trained on). For each stored pattern `x`, `-x` is a spurious pattern. But also, any linear combination of the of the learned patterns can be a spurious pattern. So let's learn a third pattern, and then see the network stabilize on a simple combination of the three patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrek = imageio.imread('shrek.jpg')\n",
    "shrek = shrek.astype(int)\n",
    "\n",
    "shrek_threshold = 200\n",
    "shrek[shrek <  shrek_threshold] = -1\n",
    "shrek[shrek >= shrek_threshold] = 1\n",
    "\n",
    "shrek[120:,:] = 1\n",
    "\n",
    "shrek = -shrek\n",
    "\n",
    "plt.imshow(shrek, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_shrek = shrek.flatten()\n",
    "\n",
    "flatlen = len(flattened_shrek)\n",
    "\n",
    "shrek_weights = np.outer(flattened_shrek,flattened_shrek) - np.identity(len(flattened_shrek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_weights = (woah_weights + deal_weights + shrek_weights) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_shrek = noisify(pattern=shrek)\n",
    "        \n",
    "plt.imshow(noisy_shrek, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_shrek = flow(noisy_shrek, average_weights, theta=theta, steps=steps)\n",
    "\n",
    "plt.imshow(recovered_shrek, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_woah = flow(noisy_woah, average_weights, theta=theta, steps=steps)\n",
    "\n",
    "plt.imshow(recovered_woah, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's make a spurious pattern. Any linear combination will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spurious_meme = shrek + deal + woah\n",
    "np.unique(spurious_meme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spurious_meme[spurious_meme > 0] = 1\n",
    "spurious_meme[spurious_meme < 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(spurious_meme, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty noisy. Only Aang, and kiiiiinda the Deal with It, are visible. Now make a noisy version of that combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_spurious_meme = noisify(pattern=spurious_meme)\n",
    "        \n",
    "plt.imshow(noisy_spurious_meme, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautifully noisy. Can barely see anything in it. But now if we start with that, and apply the weights, it should recover the spurious pattern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 100000\n",
    "\n",
    "recovered_spurious_meme = flow(noisy_spurious_meme, average_weights, theta=theta, steps=steps)\n",
    "\n",
    "plt.imshow(recovered_spurious_meme, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it sure as heck did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the Storkey Learning Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
